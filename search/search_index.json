{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction to our Project","text":"<p>Don't worry - you are not expecting to come up with a project from the ground up. Instead, we'll be working on a small sample project that you can find in this repository.</p> <p>For this first small project, we will take network device configuration as an example of version-controlled files that will be applied to the devices in our environment through our pipeline later.</p> <ul> <li>Note: In case you're reading this from the future - this repository was created for the purpose of a Partner Interactive Workshop, you can use the code as you like (subject to license) though the lab/topology sections will be redundant. There is some further documentation on how the containers are used below. *</li> </ul>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>To follow this Workshop, you should have these things:</p> <ul> <li>Docker (covered in previous Hands-On session)</li> <li>GitLab Account to save the project in a repo</li> <li>VPN Client for connection to dCloud (AnyConnect)</li> </ul>"},{"location":"#the-project","title":"The Project","text":"<p>In this project, we will work through different approaches of network device configuration and their principles.</p> <p>In the first section, we will start with the classic manual method of on-device CLI configuration - the good old reliable. Next, we will progress and examine different tools and processes that will support us - doing these changes for us. Afterwards we will shed the light on the centralized approach using a project in GitLab and start to build the foundation for our fully automated pipeline.</p> <p>In the second section we will create different stages for our pipeline that will handle different things for us. We will have different environments to deploy our changes to - to test and examine them and their impact. With all these previous building blocks, you will bring everything together: From centralized files pushed, tested, and deployed through your pipeline into your environment.</p>"},{"location":"#lab-topology","title":"Lab Topology","text":"<p>The below topology details the dCloud instance we'll used as part of this lab:</p> <ul> <li>Access: Provided via an AnyConnect VPN and will be provided by your breakout proctor.</li> <li>Ubuntu VM: Hosts the Automation components. *Catalyst VM: There are two Cat9K instances which will represent a development and production switch.</li> </ul> <p></p>"},{"location":"#automation-architecture","title":"Automation Architecture","text":"<p>For reference we have included a visual workflow of the build process - when you enter <code>docker compose up -d</code> the referenced images are searched locally, if they aren't present, they are built based on what is documented in the compose and associated DOCKERFILE.</p> <ul> <li>If you are attending our lab session, this repository will be present on the ubuntu VM documented above.</li> <li>If you are looking to enhance or extend this repository you can add additional components in a similar way as documented in the workflows below.</li> </ul> <p>We have chosen to ignore docker to build this lab and just install directly to the VM though it's important when you're designing your own stack that you understand how to interact with containers and more importantly why creating a modular system is useful when developing and separating environments. Why Containers - Cisco</p>"},{"location":"#gitlab-components","title":"Gitlab Components","text":"<p>To get started with Gitlab we have included a single GitLab instance and a runner (used to execute CI tasks.) A default_user and project are created automatically.</p> <ul> <li>User: default_user</li> <li>Password: C1sco12345</li> </ul> <p>Once you've started the containers you can access the  GUI at localhost:2080 or by clicking here.</p> <p>The build process is more complicated than some you may have seen - it requires multiple entrypoint scripts and a wrapper to ensure the user and projects are created, as well as successfully registering the runner. As we wanted a single up process each container waits for the GitLab service to be fully up by iteratively checking the health using : <code>curl $GITLAB_URL:80/-/health</code>.</p> <p>As we're running a local deployment we've chosen to create a functional runner which includes all the bits pre-built - in most production installs you're more likely going to see a pipeline to built and host a local package for this purpose and as your use cases expand you'll likely be build specific packages for each tool. An example could be a scheduled Pipeline to update an Ansible image and a 'test' image using PyATS and Robot Framework.</p> <p>If you wish to make changes to the Gitlab repository while the container is running outside the GUI you can clone the repo using the above credentials at this URL:</p> <p><code>http://localhost:2443/default_user/Default_Resources.git</code></p> <p> \\ Git Related Build Process</p>"},{"location":"#devtools-container","title":"Devtools Container","text":"<p>This container is primarily used for Hands on 1 as a development space for netmiko, PyATS and Ansible, when we work with Gitlab we're going to use Gitlab and the Gitlab Runner instead for simplicity. The container is always up and mounts the contents of the 'content/*' during build. Any changes to the content folder will not be reflected in runtime and would require a rebuild or update to the named volume.</p> <p></p> <p>DevTools Build Process</p>"},{"location":"agenda/","title":"Agenda","text":""},{"location":"agenda/#section-1-introduce-concepts-20-minutes","title":"Section 1: Introduce Concepts - 20 minutes","text":"<pre><code>-  Typical Use Cases/Personas\n-  Lab Topology - basic setup/connectivity\n\n    -  Why we need it\n    -  What it does and how it helps\n-  IaC\n    -  CLI Migration - CLI to tools/processes and job retention\n    -  Config formats - specific syntax to generic data and templates\n    -  Standardised processes - governance, triggers\n</code></pre>"},{"location":"agenda/#demo-30-minutes","title":"Demo - 30 minutes","text":"<pre><code>-  Device\n    -  Example on doing per hand (cli, netmiko)\n    -  Example of doing using a tool (ansible, pyats)\n-  Gitlab\n    -  Access Gitlab\n    -  Create basic Gitlab CI file\n    -  Inspect job details\n    -  Optional (Update description, execute against only development)\n</code></pre>"},{"location":"agenda/#section-2-dev-vs-prod-pipeline-validation-10-minutes","title":"Section 2: Dev vs Prod, Pipeline validation - 10 minutes","text":"<pre><code>-  \u2018Advanced\u2019 Pipeline Stages\n    -  Blocks : Validation, Dev deploy, testing, prod deploy\n-  Validation\n    -  Syntax/Linting\n    -  Functional\n    -  Compliance testing\n</code></pre>"},{"location":"agenda/#project-examples-30-minutes","title":"Project Examples - 30 minutes","text":"<pre><code>-  Device\n    -  Example using gitlab commit trigger\n-  Gitlab\n    -  Update CI to include change to dev environment after linting.\n-  Validation\n    -  Add linter for validation\n    -  PyATS check device state\n    -  Ensure Production execution only after successful test.\n</code></pre>"},{"location":"agenda/#closuresummary-10-minutes","title":"Closure/Summary - 10 minutes","text":"<pre><code>-  Overview of concepts\n-  Example of use cases\n-  Review of different approaches (CLI, simple tools, simple pipelines, advanced pipelines)\n-  Next Steps\n</code></pre>"},{"location":"lab_setup_connect/","title":"Lab Topology - basic setup and connectivity","text":"<p>For this Hands-On Workshop, we will use Cisco dCloud. It provides an environment that we can connect to via VPN and where our devices are running which we will configure in different ways.</p> <p>You might have to disconnect from the VPN when starting the Containers in order to bring them up successfully.</p> <p>The credentials for connecting to your dCloud Pod via VPN will be provided by your breakout proctor.</p> <p></p> <p>Our Topology features two Catalyst 9k VM Devices that have 8 interfaces each that can be configured. SSH is enabled on both and will be used to connect to these devices during our session.</p> <p>One device represents the development environment where configuration will be deployed to be tested and verified before being applied to the second device, which represents our production environment.</p> <p>In each Pod an Ubuntu VM is present to host automation components.</p>"},{"location":"lab_setup_connect/#addressing-and-access","title":"Addressing and Access","text":"<p>Catalyst VM \"dev\":</p> <ul> <li>IP: 198.18.138.11</li> <li>admin/C1sco12345</li> </ul> <p>Catalyst VM \"prod\":</p> <ul> <li>IP: 198.18.138.12</li> <li>admin/C1sco12345</li> </ul> <p>Ubuntu VM:</p> <ul> <li>IP: 198.18.133.101</li> <li>root/C1sco12345</li> <li>Can be accessed via RDP if needed</li> </ul> <p>Once you are connected to the VPN and after you installed the required tooling, you can try to access one of the Catalyst VMs via SSH like this:</p> <p><code>ssh admin@198.18.138.11</code></p> <p>Enter the password which will take you to the following prompt:</p> <p><code>Cat9kv-01#</code></p> <p>You have successfully connected to our Lab Environment and are ready to go! Please proceed to the next Step once you are advised by your proctors.</p>"},{"location":"Lab_Guide/cicd_intro/","title":"Introduction to CI/CD","text":"<p>CI/CD, shorthand for Continuous Integration/Continuous Delivery, is a practice used in software engineering to deploy a software. Before CI/CD most software was developed and then released in long cycles. A software engineer would develop a feature, throw it over a wall for QA to check that it meets all quality criteria and then, months or sometimes years later, the feature would be pushed to the public. This is not the case anymore. Companies like amazon push new code to their systems (production and testing) more then once a second and even seemingly stall pages like google update multiple times every time.</p> <p>Continuous Integration or CI is the process of continuously integrating new changes/components of the source code into the application. Instead of having one big \u201cpush day\u201d where changes are merged we continuously add changes as they are made available by the development team.</p> <p>Continuous Integration Pipelines usually start with a Source Code Management (SCM) change. New code has been pushed by a developer. What happens next depends on the organization but some common steps are</p> <ol> <li>Clone the code</li> <li>Compile the code (if needed)</li> <li>Test the code - This can include running unit, integration and static tests as well as testing for code coverage(what percentage of source code is actually tested by test cases) or memory leak tests.</li> <li>Report - Either via a dashboard, e-mail, chat or other means of communication</li> <li>(Deploy) - Code could be automatically deployed into production or staging environments - this last part then makes it a Continuous Delivery (CD) process.</li> </ol> <p>In this workshop we are going to have a look at such a CI/CD pipeline from the ground up. But in order to continuously deliver code we first need code.</p>"},{"location":"Lab_Guide/hands_on_01_section_a/","title":"All Hands on Deck","text":"<p>Now, we will start to make our first changes to our config and build the foundation for our CI/CD pipeline that we will create later.</p>"},{"location":"Lab_Guide/hands_on_01_section_a/#manual-labour","title":"Manual Labour","text":"<p>First, go ahead and make a manual change to the configuration of the interface that has been assigned to you.</p> <p>For this, you can connect to the dev Switch on 198.18.138.11 via SSH. You will find yourself on the command line with the enabled prompt <code>#</code></p> <p>Your first task is to go into the configuration mode and change the description of your assigned interface. Maybe you could mark it with your name?</p> <p>If you have trouble doing this on your own you can find the solution down below.</p> Click here to show solution Manual Configuration Example<pre><code>    Cat9kv-01#\n\n    # Let's check the current interface config\n\n    Cat9kv-01#sh run | section interface\n    interface GigabitEthernet0/0\n      vrf forwarding Mgmt-vrf\n      ip dhcp client client-id GigabitEthernet0/0\n      ip address dhcp\n      negotiation auto\n    interface GigabitEthernet1/0/1\n    interface GigabitEthernet1/0/2\n    interface GigabitEthernet1/0/3\n    ...\n\n    Cat9kv-01#\n    Cat9kv-01#conf t\n    Cat9kv-01(config)#interface GigabitEthernet 1/0/1 \n    Cat9kv-01(config-if)#description Configured manually by frewagne\n    Cat9kv-01(config-if)#end\n\n    # Now check the config of the interfaces again\n\n    Cat9kv-01#sh run | section interface\n    interface GigabitEthernet0/0\n      vrf forwarding Mgmt-vrf\n      ip dhcp client client-id GigabitEthernet0/0\n      ip address dhcp\n      negotiation auto\n    interface GigabitEthernet1/0/1\n      description Configured manually by frewagne\n    interface GigabitEthernet1/0/2\n    interface GigabitEthernet1/0/3\n    ...\n</code></pre>"},{"location":"Lab_Guide/hands_on_01_section_b/","title":"Loading your toolbelt","text":"<p>Now, you might want to get you hands less dirty than using th CLI manually every time you make a change to one device. Or what do you do when you need to make changes to multiple devices? There are plenty of tools that can help you work with your devices and automate some simple workflows already.</p>"},{"location":"Lab_Guide/hands_on_01_section_b/#install-tools","title":"Install Tools","text":"<p>We have prepared a set of tools that you can use by cloning the repository and starting you own environment in docker using compose.</p> <p>This project is extremely simple, it requires only docker to be installed on the host and this repository to be cloned or manually copied. default today is compose version 2</p> <p>You might have to change the <code>EXT_IMG_VERSION_GITLAB</code> variable in the <code>.env</code> file in this repository, depending on what's your processors architecture!</p> <ol> <li>To start the service simply run either of the following:</li> <li>(Version 1) <code>docker-compose up -d</code></li> <li>(Version 2+) <code>docker compose up -d</code></li> <li>Check containers are spinning up</li> <li>Go have a cup of tea for 5 minutes while gitlab get's ready.</li> <li>Access gitlab in your browser : http://localhost:2080</li> <li>Access devtools from cli : <code>docker exec -it engine_devtools bash</code></li> </ol> <p>Should look something like this:</p> <p></p> <p>Once all the containers are up and running, you  got yourself a great automation toolset and a GitLab instance of your own! All inclusive a GitLab runner that will take care of the execution of our pipeline later on.</p>"},{"location":"Lab_Guide/hands_on_01_section_b/#netmiko","title":"Netmiko","text":"<p>One example of the tools that can be used is Netmiko.</p> <p>We will make a short example here on how to use Netmiko to configure our interface using a python script. For this, you can use the devtools provided to you. After you accessed the devtools from cli like shown in step 5 above, you are ready to go.</p> <p>If you want to do it locally, will you need to have Python installed and to install netmiko, simply use pip: <code>$ pip install netmiko</code></p> <p>We will use the following small python script, which is also located on the devtools container:</p> <pre><code>    from netmiko import ConnectHandler\n\n    cat_dev = {\n        'device_type': 'cisco_xe',\n        'host':   '198.18.138.11',\n        'username': 'admin',\n        'password': 'C1sco12345',\n    }\n    net_connect = ConnectHandler(**cat_dev)\n    # Execute Show Command\n    output = net_connect.send_command('show run | section interface')\n    print(output)\n    print(\"\\n\")\n    config_commands = [ 'interface GigabitEthernet 1/0/1',\n                        'description Configured with netmiko by frewagne'\n                    ]\n    # Use your assigned interface to configure!\n    output = net_connect.send_config_set(config_commands)\n    print(output)\n    print(\"\\n\")\n    output = net_connect.send_command('show run | section interface')\n    print(output)\n</code></pre> <p>Make sure to use the interface number that is assigned to you!</p> <p>Execute the python script using <code>$ python3 netmiko_interface_description.py</code>.</p> <p>Once the script has been successfully executed, we can check the current interface config:</p> <pre><code>    Cat9kv-01#sh run | section interface\n    interface GigabitEthernet0/0\n      vrf forwarding Mgmt-vrf\n      ip dhcp client client-id GigabitEthernet0/0\n      ip address dhcp\n      negotiation auto\n    interface GigabitEthernet1/0/1\n      description Configured with netmiko by frewagne\n    interface GigabitEthernet1/0/2\n    interface GigabitEthernet1/0/3\n    ...\n</code></pre>"},{"location":"Lab_Guide/hands_on_01_section_c/","title":"All hands on GitLab","text":"<p>To get started with GitLab, we will make use of all of the containers that you spun up previously.</p> <p>You can now access your GitLab instance under <code>localhost:2080</code> in your browser and log in with the specified credentials: <code>default_user/C1sco12345</code> </p> <p>Once you successfully authenticated, you will see an existing project called <code>Default Resources</code> in which you will find a folder structure named <code>Ansible/playbooks</code> where we will store the files that will make up our device configuration.</p> <p>Now you have the foundation for the pipeline! In this project repository we will store our files and add a CI file which can be interpreted by GitLab and is the collection of stages and tasks that will make up our pipeline in the end. Next we will add our configuration template to our repository. Storing it centrally in the repo enables tracking of changes, collaborative work and rollback of commits if needed. After our playbook is stored, we will create the CI file called <code>.gitlab-ci.yml</code>. Here we will describe the procedure of our pipeline. We will start with a basic dummy skeleton:</p> <pre><code>  ---\n  dummy-job:\n    script:\n      - echo \"This pipeline is triggered successfully!\"\n</code></pre> <p>Once you commit this CI file in the repository, the pipeline will get triggered for the first time. Make sure to check if the pipeline was successful and the command was executed through the runner.</p> <p>You will now start to write your own CI file to configure the <code>dev</code> Switch using Ansible.</p> <p>The Ansible playbooks can be found in the <code>Ansible</code> subdirectory and executed from there. Keep this in mind while building your pipelines syntax.</p> <p>Playbooks can be excuted through Ansible with the command <code>ansible-playbook</code> followed by the <code>-i</code> option, specifying the inventory file - which is already prepared for you. For applying additional variables you can use the <code>-e</code> option. The <code>interface_update.yml</code> playbook resides in the <code>playbooks</code> subdirectory. Make sure to check the files to understand their use and structure.</p> <p>Here is a base syntax that you will need to fill in the <code>***</code> placeholders with the correct syntax:</p> Ansible Pipeline example<pre><code>---\ndeploy_infra:\n  script:\n    - cd ***\n    - ansible-playbook -i *** -e 'devices=***' playbooks/***.yml\n</code></pre> <p>Please note that the name of the job should be <code>deploy_infra</code>!</p> <p>If you are having trouble with this task, we have prepared an example in the dropdown below.</p> Click here to show solution Ansible Pipeline solution<pre><code>---\ndeploy_infra:\n  script:\n    - cd Ansible\n    - ansible-playbook -i inventory -e 'devices=development' playbooks/interface_update.yml\n</code></pre>"},{"location":"Lab_Guide/hands_on_02_section_a/","title":"Hands on 2: Linting &amp; Stages","text":""},{"location":"Lab_Guide/hands_on_02_section_a/#linting","title":"Linting","text":"<p>As discussed in the main room - linting is a term used to describe tools which make sure we haven't made some serious tyypos in our code/configuration. Depending on the tool used there are additional useful features to ensure adherence to a particular standard as an example most IDEs include PyFlakes which looks for syntactical errors and some recommendations based on agreed standards. You can enhance this functionality with tools such as Flake8 which acts as a wrapper for specifications (in this case PEP8 which aims to improve readability of code.</p> <p>For some of us linting might sound like an optional step but in reality ensuring readability and simplicity is essentially when adopting new technologies or approaches. So do it!! :)</p> <p>In this task we're going to incorporate a job to lint our Ansible folder to make sure we've no typos!</p> <pre><code>yamllint:\n  stage: validate\n  image: registry.gitlab.com/pipeline-components/yamllint:latest\n  script:\n    - yamllint Ansible\n</code></pre> <p>This is a pretty simple example where we're executing a single bash command, we used the <code>yamllint</code> image to build a container which linted our Ansible folder and returned success/failure. The exit code is important here as we'll be using it as a conditional step in a future task. To make things really clear i've documented them below:</p> <pre><code>Job name: -&gt; yamllint:\nStage name: -&gt;  stage: validate\nImage name: -&gt; image: registry.gitlab.com/pipeline-components/yamllint:latest\nAction: -&gt;  script:\nCLI execution:  -&gt;  - yamllint Ansible\n</code></pre> <p>In production pipelines you should expect to see multiple lint stages depending on the specific tool/language used. In our demo example we could have used both <code>yamllint</code> and <code>ansible-lint</code> which contain different rule packages though we chose to use one for simplicity.</p>"},{"location":"Lab_Guide/hands_on_02_section_a/#task-add-lint-job","title":"Task - Add lint job","text":"<p>The first step of this task is pretty simple:</p> <ol> <li>Copy the below into your gitlab ci file, commit the file.</li> </ol> <pre><code>yamllint:\n  image: registry.gitlab.com/pipeline-components/yamllint:latest\n  script:\n    - yamllint Ansible\n</code></pre> <ol> <li>Check to see what happened in the pipeline (CI/CD &gt; Pipelines)</li> <li>Add some junk data to the top of the <code>interfaces_update.yml</code> file and hopefully the yamllint job failed.</li> <li>Remove your junk data and check to see success.</li> </ol> <p>That's it for now, but we'll be using the fail/success outcomes in future tasks.</p>"},{"location":"Lab_Guide/hands_on_02_section_a/#stages","title":"Stages","text":"<p>Stages are a relatively simple concept in that they contain multiple jobs and represent a logical phase in our workflow, common examples include build, validate, test, deploy etc.. They're useful for isolating functionality and can be used to restrict execution based on a previous phase (as we will see later.) A common use case is don't deploy unless all your tests and build phases have passed.</p> <p>If no stages are specified, a 'test' stage is implied; so that we can start segmenting our Pipeline we'll create two stages (validate, deploy) at the top of our ci file.</p> <pre><code>---\nstages:\n  - validate\n  - deploy\n</code></pre> <p>We tested before the outcome of linting above, so now we'll test an  For both our jobs <code>yamllint</code> and <code>deploy_infra</code> you need to specify with which stage the job is associated as below:</p> <pre><code>yamllint:\n  ...\n  stage: validate\n  ...\n\ndeploy_infra:\n  ...\n  stage: deploy\n  ...\n</code></pre>"},{"location":"Lab_Guide/hands_on_02_section_a/#task-add-stages","title":"Task - Add Stages","text":"<ol> <li>Update your ci file with stages as above</li> <li>Inspect the Pipeline to see separation of jobs.</li> </ol>"},{"location":"Lab_Guide/hands_on_02_section_a/#conditional-phases","title":"Conditional Phases","text":"<p>There are many ways to create logic between jobs within Gitlab CI/CD, the simplest uses the <code>needs:</code> parameter to specify that the execution of a job relies on the successful execution of a previous job. In our case we want to ensure that the deploy_infra job doesn't execute until the lint job has successfully completed.</p>"},{"location":"Lab_Guide/hands_on_02_section_a/#task-conditional-execution","title":"Task - Conditional Execution","text":"<ol> <li>Specify that deploy_infra requires yamllint to execute before it can begin.</li> </ol> Click here to show solution Conditional Execution<pre><code>deploy_infra:\n  stage: deploy\n  script:\n    - cd Ansible\n    - ansible-playbook -i inventory -e 'devices=all' playbooks/interface_update.yml\n  needs: \n    - yamllint    \n</code></pre>"},{"location":"Lab_Guide/hands_on_02_section_a/#branch-specific-policies","title":"Branch Specific Policies","text":"<p>As a project expands it in nearly all cases becomes necessary to create branches when working across diverse teams or infrastructure - making changes directly to master is in fact considered bad practice as it increases complexity when resolving <code>merge conflicts</code> and approving <code>pull requests.</code> Typically we would recommend creating function specific branches to avoid these issues.</p> <p>In most production instances would will see increased restrictions around branches that are permitted to make changes to 'protected' resources - a simple example is that you can only make changes to production from the master or 'prod' branch, and you can only push changes to that branch after review from your peers.</p> <p>In our example we're going to use the <code>only:</code> parameter to ensure that only changes from master can execute this job.</p>"},{"location":"Lab_Guide/hands_on_02_section_a/#task-branch-policy","title":"Task - Branch policy","text":"<ol> <li>Similar to the needs parameter, update deploy_infra to only execute from the master branch.</li> </ol> Click here to show solution Conditional Execution<pre><code>deploy_infra:\n  stage: deploy\n  script:\n    - cd Ansible\n    - ansible-playbook -i inventory -e 'devices=all' playbooks/interface_update.yml\n  needs: \n    - yamllint    \n  only:\n    - master\n</code></pre>"},{"location":"Lab_Guide/hands_on_02_section_a/#conclusion","title":"Conclusion","text":"<p>Great job for getting this far, I hope you'll takeaway the idea that testing the quality of our changes in advance is a good thing and adding enhancements like linting and conditional changes is a simple process with great rewards! Please move on to the next section in the navigation bar on the left.</p>"},{"location":"Lab_Guide/hands_on_02_section_b/","title":"Development vs Production &amp; Testing","text":"<p>Many customers are stuck assessing which areas to prioritize when beginning to automate - a common practice is to prioritize the areas which you most frequently perform. Testing is a great starting point as when we use tools such as <code>robot framework</code>, we can abstract the complexities of specific device implementations with abstracted test suites. To give you an example, L3 Routing is an inherently complex area to test; it requires understanding of the existing and desired state, and involves many components with different interfaces (Cisco, Juniper, Arista etc.. ) Before tackling some of these monster jobs, it's usually a good idea to focus on simple and repeatable tasks. For our demo we chose a simple description change and test to demonstrate intent - you could also choose validating attributes like uptime, software version or interface errors to demonstrate to other stakeholders why automating these manual tasks is a great idea!</p> <p>Like L3 Routing this topic is also a monster but feel free to speak with your proctors who i</p> <p>Similar to the previous linting phase we're going to add a job to execute our pyATS test from Section 1 - again in practice you're going to see many different types of testing in your pipelines.</p>"},{"location":"Lab_Guide/hands_on_02_section_b/#specific-devices","title":"Specific devices","text":"<p>In our lab we're using two switches <code>cat9kv01-dev</code> and <code>cat9kv02-prod</code> as symbolic representations of development and production - depending on scale you may create separate stages for these groups; as we're demonstrating concepts we'll keep things simple. To ensure that any serious issues don't affect production, we're going to separate the execution of the ansible playbook to first target only the development hosts, execute a test and based on the success of the test execute in production.</p> <ol> <li>Replace the deploy stage with two covering <code>development</code> and <code>production.</code></li> <li>Rename deploy_infra to deploy_dev, assigned to correct stage and update the environment variable within the ansible-playbook command to run only on dev devices (check Ansible Inventory file in case you get stuck)</li> </ol> Click here to show solution <pre><code>deploy_dev:\n  stage: development\n  script:\n    - cd Ansible\n    - ansible-playbook -i inventory -e 'devices=development' playbooks/interface_update.yml\n  needs: \n    - yamllint\n  only:\n    - master\n</code></pre> <ol> <li>Create a production job called <code>deploy_prod</code> assigned to correct stage, which targets only production devices.</li> </ol> Click here to show solution <pre><code>deploy_prod:\n  stage: production\n  script:\n    - cd Ansible\n    - ansible-playbook -i inventory -e 'devices=production' playbooks/interface_update.yml\n  needs: \n    - yamllint\n  only:\n    - master    \n</code></pre>"},{"location":"Lab_Guide/hands_on_02_section_b/#task-adding-a-test-job","title":"Task - Adding a test job","text":"<p>You should be a whizz at creating jobs in ci files by now so we've included them below, when you choose to edit the <code>.gitlab-ci.yml</code> file you are given an option to edit in 'Pipeline Editor' which allows you to visualize the change. </p> <ol> <li>Add a stage for test.</li> <li>Create two jobs <code>test_dev</code> and <code>test_prod</code> , update the pyATS command as appropriate.</li> <li>Update <code>needs:</code> on both deploy jobs as appropriate to ensure tests follow deploy actions and production deploy doesn't execute until test_dev has completed.</li> <li> <p>Experiment with these features to make sure your change creates the new stage and job.</p> </li> <li> <p>pyATS syntax : <code>python3 pyATS_test.py dev 1</code> = development switch, interface 1.</p> </li> </ol> pyATS example<pre><code>test_XXX:\n  stage: XXX\n  script:\n    - cd pyATS\n    - 'Insert pyATS commands'\n  needs:\n    - XXXX\n</code></pre> <p>NOTE: update your interface to the one which your proctor provides for Section 2 *</p> <p>The complete CI file can be found here: Complete CI File</p>"},{"location":"Lab_Guide/hands_on_02_section_b/#gold-star-exercises","title":"Gold Star Exercises","text":"<ul> <li>Docs as Code: Check out of Github Actions file and try and figure out how we've made this lab guide!</li> <li>Build a revert model for failed stages</li> </ul>"}]}